\documentclass[a4paper,NoNotes,GeneralMath]{stdmdoc}

\newcommand{\mtrvec}[2]{\left[ {#1} \right]_{#2}}
\newcommand{\virg}{\operatornamewithlimits{,}}
\newcommand{\mtrapp}[3]{\underset{ {{#2} \rar {#3}} }{ \left[ {#1} \right]}}
\newcommand{\revmtrapp}[3]{\underset{ {{#3} \leftarrow {#2}} }{ \left[ {#1} \right]}}
\newcommand{\Vand}{\mbox{Vand }}

\newcommand{\Help}{{\color{green} AIUTO! DA SCRIVERE}}


\begin{document}
	\title{Fatti Utili di GAAL}
	\autodate

	\section*{Notazione}
	\begin{itemize}
		\item $V$ spazio vettoriale di dimensione $n$ su un campo $\bbK$
		\item $f$ endomorfismo di $V$
		\item $\lambda$ autovalore di $f$
		\item $\chi_f(t)$ è il polinomio caratteristico di $f$
		\item $m_f(t)$ è il polinomio minimo di $f$
		\item $J_f$ la forma di Jordan di $f$
		\item $M_\lambda$ la sottomatrice di $J_f$ relativa all'autospazio generalizzato $V_\lambda'$
		\item $\varphi$ prodotto scalare su $V$
		\item $q$ forma quadratica su $V$
		\item $\sigma(\varphi) = (i_{+}, i_{-}, i_0)$ la segnatura di $\varphi$ (il campo deve essere $\bbR$)
		\item $\omega(\varphi)$ l'indice di Witt di $\varphi$
		\item $\Psi^{V} : V \rightarrow V^{**}$ è l'isomorfismo canonico tra uno spazio vettoriale ed il suo biduale
		\item $H$, $L$ sottospazi affini, $W_H$, $W_L$ le loro giaciture
		\item Dato $\phi$ prodotto scalare, definiamo $\phi_y: V \rightarrow \bbK \tc x \mapsto \phi_y(x) = \phi(x,y)$ e $F_\phi: V \rightarrow V^{*} \tc y \mapsto \phi_y$
	\end{itemize}
	
	\section*{Sistemi Lineari}
	\Altro{Cosa si conserva nella riduzione di Gauss} \\
	Sia $Ax = b$ un sistema lineare e $Sx = c$ la sua ridotta a scala
	\begin{itemize}
		\item L'insieme delle soluzioni di $Ax = b$ è uguale a quello delle soluzioni di $Sx = c$
		\item $\Ker A = \Ker S$
		\item $\Rk A = \Rk S$ (ma in generale $\Img A \neq \Img S$, solo le loro dimensioni sono uguali)
		\item Siano $S^{j_1}, \ldots, S^{j_r}$, dove $r = \Rk S$, le colonne corrispondenti ai pivot di $S$; allora $\{A^{j_1}, \ldots, A^{j_r}\}$ è una base di $\Img A$
	\end{itemize}
	\Altro{Risolubilità} \\
	Attraverso considerazioni piuttosto stupide si nota che un sistema $Ax = b$ è risolubile $\sse$ $Sx = c$ è risolubile $\sse$ considerando la matrice $S' = \left( S \mid c \right)$ tutti i suoi pivot stanno nelle colonne di $S$ (ovvero $c$ non contiene alcun pivot)

	\section*{Rappresentazione Cartesiana e Parametrica}
	\Altro{Passaggio da Parametrica a Cartesiana} \\
	Abbiamo il sottospazio nella forma $\Img B$ con $B$ matrice. Allora consideriamo $B' = \left( B \mid X \right)$ con $X$ colonna "formale" delle coordinate $\left( \begin{array}{c} x_1 \\ \vdots \\ x_n \end{array} \right)$. A questo punto riduciamo a scalini la matrice $B'$, facendo tutti i passaggi anche sulla colonna "formale". Sappiamo ora che le righe in fondo alla matrice $S'$ (la ridotta a scala di $B'$) sono nulle su tutta $S$ (la ridotta a scala di $B$), quindi anche gli ultimi termini in $X_{S'}$ (ultima colonna di $S'$) devono essere nulli (altrimenti il sistema non è risolubile). Abbiamo così trovato le equazioni cartesiane che definiscono il nostro sottospazio.

	\Altro{Passaggio da Cartesiana a Parametrica} \\
	Siccome abbiamo il sottospazio nella forma $\Ker A$ con $A$ matrice, l'unica cosa da fare è risolvere il sistema lineare associato, prendere una base del sottospazio trovato e mettere i vettori in questione (che stanno nello spazio delle coordinate, per cui è lecita l'identificazione tra vettori e matrici colonna) come colonne nella nuova matrice della forma parametrica.

	\section*{Applicazioni Lineari}
	\begin{itemize}
		\item $f$ lineare è iniettiva $\sse \Ker f = \fucknullset$
		\item Se $f: V \rar W$ è lineare e $\{v_1, \ldots, v_n\}$ sono linearmente indipendenti, allora anche $\{f(v_1), \ldots, f(v_n)\}$ sono linearmente indipendenti
		\item ({\bf Formula delle Dimensioni}) $f: V \rar W$ lineare $\implies \Dim V = \Dim \Img f + \Dim \Ker f$
		\item $f: V \rar W$ lineare, con $\Dim V = \Dim W$. Allora $f$ è iniettiva $\sse$ è surgettiva
		\item ({\bf Determinazione Univoca}) Un'applicazione lineare è univocamente determinata da ciò che fa su una base, ovvero sia $\cB = \{b_1, \ldots, b_n\}$ base di $V$ e $w_1, \ldots, w_n$ vettori di $W$. Allora $\exists ! f:V \rar W$ lineare $\tc f(b_i) = w_i \quad \forall i$
		\item ({\bf Formula di Grassmann}) $U, W$ sottospazi di $V$. Allora $\Dim (U+W) = \Dim U + \Dim W - \Dim (U\cap W)$
	\end{itemize}

	\section*{Rappresentazione in Base}
	\begin{itemize}
		\item Data una base $\cB$ = $\{b_1, \ldots, b_n\}$, ad ogni vettore $v$ è naturalmente associata la $n$-upla delle sue componenti (sotto forma di matrice $n\times 1$). L'isomorfismo di passaggio in coordinate induce una corrispondenza tra $V$ e $\bbK^n$ che sono e restano però due spazi diversi. \\ \hsystem{\mtrvec{v}{\cB}: V \rar \kM(n, 1, \bbK)}{v = \sum_{i} a_i b_i \quad \mapsto \quad \left( \begin{array}{c} a_1 \\ \vdots \\ a_n \end{array} \right)_{\cB}}
		\item Introduciamo inoltre la rappresentazione in base di un'applicazione lineare $f: V \rar W$, usando $\cB$ base di $V$ e $\cT$ base di $W$: rappresentiamo $f$ con la matrice $\mtrapp{f}{\cB}{\cT}$ che ha come colonna $i$-esima la rappresentazione del vettore $f(b_i)$ come matrice colonna $\mtrvec{f(b_i)}{\cT}$
		\item Valutazione: $\mtrvec{f(v)}{\cS} = \revmtrapp{f}{\cB}{\cS} \mtrvec{v}{\cB}$
		\item Composizione di funzioni: $\revmtrapp{g\circ f}{\cB}{\cD} = \revmtrapp{g}{\cR}{\cD} \revmtrapp{f}{\cB}{\cR}$
		\item Cambio di base: $\revmtrapp{f}{\cB}{\cB} = \revmtrapp{\Id}{\cD}{\cB} \revmtrapp{f}{\cD}{\cD} \revmtrapp{\Id}{\cB}{\cD}$
		\item Le matrici del cambio di base sono una l'inversa dell'altra: $\revmtrapp{\Id}{\cB}{\cR} = \left(\revmtrapp{\Id}{\cR}{\cB}\right) ^ {-1}$
		\item ({\bf Matrici $\leftrightarrow$ Applicazioni}) Inoltre le applicazioni lineari $f: \bbK^m \rar \bbK^n$ sono in corrispondenza biunivoca con le $\kM(n, m, \bbK)$ attraverso l'isomorfismo di passaggio in coordinate (ovvero ogni matrice induce naturalmente un'applicazione lineare $\bbK^m \rar \bbK^n :: v \mapsto L_M v$ attraverso la moltiplicazione tra matrici e l'identificazione dei numeri risultati con lo spazio delle coordinate)
	\end{itemize}
	\Achtung Non confondete i vettori (elementi dello spazio vettoriale $V$) con la $n$-upla di numeri che viene usata per rappresentarli. Molto spesso si confondono le cose perché le matrici $n\times 1$ usate per rappresentare i vettori dello spazio vettoriale delle coordinate $\bbK^n$ (visto come spazio vettoriale su $\bbK$, i cui vettori si possono scrivere come $(a_1, \ldots, a_n)$) hanno come numeri nelle celle gli stessi $a_1, \ldots, a_n$ perché vengono solitamenti scritti usando le coordinate dei vettori rispetto alla base standard di $\bbK$.

	\section*{Rango}
	\begin{itemize}
		\item Il rango di una matrice è lo stesso sia per righe che per colonne (ovvero il numero delle righe linearmente indipendenti è uguale al numero delle colonne linearmente indipendenti)
		\item ({\bf Minori Invertibili}) Il rango di una matrice corrisponde al massimo ordine di un minore invertibile della matrice
		\item ({\bf Minori Orlati}) $\Rk A = k \sse \exists M$ minore invertibile di $A$ di ordine $k$ tale che tutti i suoi orlati sono non invertibili
		\item $\Rk A = \Rk {}^tA$
		\item $\Dim \Img (g\circ f) \le \min(\Dim \Img(f), \Dim \Img(g))$
		\item Un'operazione elementare eseguita sulle righe di una matrice può essere vista come la moltiplicazione a sinistra per un'opportuna matrice "elementare"
		\item ({\bf Calcolo dell'Inversa}) Per trovare l'inversa di una matrice $A$ supposta invertibile occorre scrivere la matrice $B = \left( A \mid I \right)$ e operare sulla matrice $B$ facendo in modo che essa sia a sinistra (ovvero nella parte che contiene $A$) ridotta a scalini in maniera forte. Successivamente si moltiplicano le righe in modo da ottenere l'identità sulla sinistra. Nella parte destra di $B$ si può ora leggere la matrice inversa di $A$
	\end{itemize}

	\section*{SD-equivalenza}
	\begin{itemize}
		\item Invariante completo di SD-equivalenza è il rango, in particolare, $\forall f:V \rar W$ lineare, $\Dim V = n$, $\Dim W = p$, $\Rk f = r \quad \exists \cB$ base di $V$, $\exists \cS$ base di $W$ tale che $\mtrapp{f}{\cB}{\cS} = \left( \begin{array}{c|c} I_r & 0 \\ \hline 0 & 0 \\ \end{array} \right)$
	\end{itemize}
	
	\section*{Determinante}
	\begin{itemize}
		\item ({\bf Formula con le Permutazioni}) $\Det A = \sum_{\sigma \in S_n} A_1^{\sigma_1} \cdot \ldots \cdot A_n^{\sigma_n}$
		\item ({\bf Sviluppo di Laplace}) $\Det A = \sum_{j=1}^{n} (-1)^{i+j} [A]_{ij} \Det(\overline{A_{ij}})$ dove con $\overline{A_{ij}}$ si intende il complemento algebrico di $A_{ij}$, ovvero la matrice ottenuta da $A$ togliendo la $i$-esima riga e la $j$-esima colonna
		\item ({\bf Riduzione a Scala}) Il determinante di $A$ si può calcolare anche riducendola a scala: Sia $S$ la matrice ridotta a scala ottenuta da $A$ attraverso $m$ operazioni di primo tipo (scambi di righe) e $k$ di terzo tipo (sommare ad una riga un multiplo di un'altra). Allora $\Det(A) = (-1)^{m} \Det(S)$. \\ Inoltre sia $S'$ la ridotta a scala forte, ovvero eseguendo solo operazioni di terzo tipo si portiamo $S$ nella forma $S' = \left( \begin{array}{ccc} a_1 & & 0 \\ & \ddots & \\ 0 & & a_n \end{array} \right)$ e ora abbiamo $\Det S = \Det S' = a_1 \cdot \ldots \cdot a_n$
		\item ({\bf Teorema di Binet}) $\Det AB = \Det A \cdot \Det B$
		\item $A$ è invertibile $\sse \Det A \neq 0$
		\item ({\bf Regola di Cramer}) $AX = B$ un sistema lineare quadrato con $n$ equazioni in $n$ incognite, $\Det A \neq 0$. Allora la sua unica soluzione è $Y = \left( \begin{array}{c} y_1 \\ \vdots \\ y_n \end{array} \right)$, dove $y_i = \frac{\Det B(i)}{\Det A}$, con $B(i) = \left( A^{1} \middle| \ldots \middle| \underbrace{B}_{i-\text{esima colonna}} \middle| \ldots \middle| A^{n} \right)$
		\item ({\bf Calcolo dell'Inversa}) Se $A$ è invertibila, allora si ha $[A^{-1}]_{ij} = (-1)^{i+j} \frac{\Det (\overline{A_{ji}})}{\Det (A)}$
		\item ({\bf Determinate di Vandermonde}) $\lambda_1, \ldots, \lambda_n \in \bbK$. $$\Vand(\lambda_1, \ldots, \lambda_n) = \left( \begin{array}{ccccc} 1 & \lambda_1 & \lambda_1^2 & \cdots & \lambda_1^{n-1} \\ 1 & \lambda_2 & \lambda_2^2 & \cdots & \lambda_2^{n-1} \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 1 & \lambda_n & \lambda_n^2 & \cdots & \lambda_n^{n-1} \\ \end{array} \right)$$ è tale che $\Det \Vand(\lambda_1, \ldots, \lambda_n) = \prod_{i<j} (\lambda_j - \lambda_i)$
	\end{itemize}
	
	\section*{Uguaglianze a Caso}
	\begin{itemize}
		\item ${}^t(AB) = {}^tB {}^tA$
		\item $(AB)^{-1} = B^{-1}A^{-1}$
		\item $({}^tA)^{-1} = {}^t(A^{-1})$
		\item $\Tr (AB) = \Tr (BA)$
		\item $\Det ({}^tA) = \Det (A)$
		\item $\Det (A^{-1}) = (\Det A)^{-1}$
	\end{itemize}

	\section*{Autovalori ed Autovettori}
	\begin{itemize}
		\item $\lambda$ è autovalore per $A \sse \Det(A-\lambda I) = 0$
		\item Se $A = \left( \begin{array}{c|c} M & N \\ \hline 0 & P \\ \end{array} \right)$ allora $\Det A = \Det M \cdot \Det P$, ovvero $\chi_A(t) = \chi_M(t) \cdot \chi_P(t)$
		\item Il determinante è il prodotto degli autovalori
		\item La traccia è la somma degli autovalori
		\item $\lambda \in \Sp(f)$. Allora $1 \le \mu_g(\lambda) \le \mu_a(\lambda) \le \Dim V$
	\end{itemize}

	\section*{Diagonalizzabilità e Forma di Jordan}
	\begin{itemize}
		\item ({\bf Diagonalizzabilità}) $f$ è diagonalizzabile $\sse \chi_f(t)$ è completamente fattorizzabile e, per ogni $\lambda$ autovalore, $\mu_a(\lambda) = \mu_g(\lambda) \sse m_f(t)$ è square-free (ovvero non ha radici doppie)
		\item ({\bf Triangolabilità}) $f$ è triangolabile $\sse \chi_f(t)$ è completamente fattorizzabile in $\bbK$
		\item ({\bf Hamilton-Cayley}) $m_f(t) \mid \chi_f(t)$. Inoltre tutti i fattori del polinomio caratteristico sono contenuti nel polinomio minimo (hanno un esponente più basso ma mai nullo)
		\item Se $q(t) \in I_f$ allora $m_f(t) \mid q(t)$. Quindi gli autovalori di $f$ devono essere radici del polinomio $q(t)$	
		\item ({\bf Nilpotenza}) $f$ è nilpotente $\sse \chi_f(t) = \pm t^n$
		\item $M_\lambda$ ha dimensione $\mu_a(\lambda)\times\mu_a(\lambda)$.
		\item ({\bf Esponenti del polinomio minimo}) La massima taglia dei blocchetti di Jordan in $M_\lambda$ è uguale all'esponente del fattore $(t-\lambda)$ nel polinomio minimo di $f$
		\item ({\bf Numero di blocchetti di Jordan}) $M_\lambda$ contiene un numero di blocchetti di Jordan pari a $\mu_g(\lambda)$. Più precisamente $$\Dim\Ker (f-\lambda\ \Id)^k-\Dim\Ker (f-\lambda\ \Id)^{k-1}$$ è il numero di blocchetti di Jordan di taglia {\it almeno} $k$
		\item ({\bf $A \sim {}^tA$}) Ogni matrice $A$ è simile alla sua trasposta ${}^tA$ (hanno le stesse dimensioni dei $\Ker$ negli invarianti di similitudine, ovvero $\Ker (A-\lambda I)^k = \Ker ({}^tA-\lambda I)^k$)
		\item ({\bf Base Ciclica}) Esiste una base ciclica per $f \sse m_f(t) = \pm \chi_f(t)$
		\item ({\bf Decomposizione degli $f$-invarianti}) Se $W$ è $f$-invariante, allora $\chi_{f\mid_W}(t) \mid \chi_f(t)$. Inoltre $I_f \subseteq I_{f\mid_W}$ da cui $m_{f\mid_W}(t) \mid m_f(t)$. \\ In particolare, per decomposizione primaria, se $\chi_{f\mid_W}(t) = (\lambda_1-t)^{\alpha_1}\ldots(\lambda_h-t)^{\alpha_h}$ possiamo scrivere $W$ come $$W = \Ker (f\mid_W-\lambda_1\ \Id)^{\alpha_1} \oplus \ldots \oplus \Ker (f\mid_W-\lambda_h\ \Id)^{\alpha_h}$$ Ovvero $W$ si scrive come $W$ = $W\cap V_{\lambda_1}' \oplus \ldots \oplus W\cap V_{\lambda_h}'$ \\
		Supponiamo ora di sapere che $m_f(t) = \pm \chi_f(t)$, ovvero che, jordanizzando $f$, si ottiene un solo blocco di Jordan per ogni autovalore. Dimostriamo allora che esistono un numero finito di sottospazi $W$, $f$-invarianti e di dimensione fissata. \\
		Indichiamo con $m = \Dim W$. Siano $v_1, \ldots, v_k, v_{k+1}, \ldots, v_{r}$ i vettori della base di Jordan di $W \cap V_{\lambda}'$. Restringendoci ora ad un solo $W \cap V_{\lambda}'$: se $a_kv_k+\ldots+a_1v_1 \in W$ allora (applicando $f$ e togliendo $\lambda$ volte il vettore originario) anche $a_kv_{k-1}+\ldots+a_2v_1 \in W$. Applicando lo stesso ragionamento un po' di volte si ottiene che $v_1 \in W$ da cui, procedendo a ritroso, anche $v_2, v_3, \ldots, v_k \in W$. Ovvero se $W$ contiene una combinazione di vettori della base di Jordan, allora contiene anche tutti quelli precedenti (siamo nell'assunzione che per ogni autovalore esista un solo blocco di Jordan). Per dimensioni si finisce.
	\end{itemize}

	\section*{Prodotti Scalari e Forme Bilineari}
	\Achtung \\ Per questa sezione assumiamo che $\bbK$ sia un campo a caratteristica diversa da $2$
	\begin{itemize}
		\item ({\bf Formula di Polarizzazione}) $\varphi(u,v) = \frac{q(u+v)-q(u)-q(v)}{2}$. In particolare se tutti i vettori sono isotropi $\varphi \equiv 0$
		\item $\Rad\varphi$ e $\Rad\varphi\mid_W$ non hanno alcuna relazione sensata tra loro
		\item $\varphi$ definito (o semidefinito) $\implies \varphi\mid_W$ definito (o semidefinito)
		\item Il rango di $\varphi$ è invariante per congruenza così come, su $\bbR$, il segno del determinante. Al contrario, il determinante in generale cambia (cioè non serve a un cazzo).
		\item $V = U \oplus \Rad\varphi \implies \varphi\mid_U$ è non degenere
		\item ({\bf Anisotropia}) Se $\bbK = \bbR$ allora $\varphi$ anisotropo $\sse \varphi$ è definito \\ Se $\bbK = \bbC$ allora $\varphi$ anisotropo $\sse \Dim(V) = 1$ e $\varphi$ non degenere
	\end{itemize}
	\Altro{Fatti da conoscere}
	\begin{itemize}
		\item Se $\varphi$ è non degenere, allora $\omega(\varphi) \le \frac{\Dim V}{2}$
		\item Sia $(V, \varphi)$, $\varphi$ non degenere, $n = \Dim V$ \\ Se $\bbK = \bbC$, $\omega(\varphi) = \floor{\frac{n}{2}}$ \\ Se $\bbK = \bbR$, $\omega(\varphi) = \min\{i_{+}, i_{-}\}$
		\item ({\bf Teorema di Riesz}) $F_\phi$ è un isomofismo (canonico) $\sse \phi$ è non degenere. Ogni $g \in V^{*}$ è quindi $\phi$-rappresentabile in modo unico
	\end{itemize}

	\section*{Proprietà di Ortogonale, Annullatore, $\phi$-Rappresentabilità e Aggiunto}
	\Altro{Ortogonale} \\ Siano $S, T \subseteq V$ (non necessariamente sottospazi)
	\begin{itemize}
		\item $S^\bot$ è un sottospazio di $V$
		\item $S \subseteq T \implies T^\bot \subseteq S^\bot$ (rovescia le inclusioni)
		\item $S^\bot = {\Span(S)}^\bot$
		\item $S \subseteq {S^\bot}^\bot$ (in generale non sono uguali)
		\item $(U + W)^\bot = U^\bot \cap W^\bot$
		\item $U^\bot + W^\bot \subseteq (U \cap W)^\bot$
		\item $\Dim(W^\bot) = \Dim(V) - \Dim(W) + \Dim(W \cap \Rad\varphi)$ \\ (Se $\varphi$ è non degenere, allora vale $W \cap \Rad \varphi = \fucknullset$ ma non è detto che $W \oplus W^\bot = V$ né che $W \cap W^\bot = \fucknullset$)
		\item $\Dim(W^\bot) + \Dim(W) \ge \Dim(V)$
		\item $\varphi\mid_W$ è non degenere $\sse V = W \oplus W^\bot$
	\end{itemize}

	\Altro{Annullatore} \\ Sia $S \subseteq V$.
	\begin{itemize}
		\item $\Ann S$ è sottospazio vettoriale di $V^{*}$
		\item $S \subseteq T \implies \Ann T \subseteq \Ann S$ (ovvero rovescia le inclusioni)
		\item $U$ sottospazio vettoriale di $V$, $\Dim U = k \implies \Dim \Ann U = n - k$
		\item $\Ann S = \Ann (\Span S)$
		\item $f \in V^{*}$, $\Ann f = \Psi^{V}(\Ker f)$
		\item $U$ sottospazio vettoriale di $V$, $\Ann \Ann U = \Psi^{V} (U)$
	\end{itemize}

	\Altro{Morfismo di Rappresentazione}
	\begin{itemize}
		\item $F_{\phi}$ è lineare
		\item $\Ker F_{\phi} = \Rad \phi$
		\item $\Img F_{\phi} = \Ann \Rad \phi$
		\item $U$ sottospazio di $V$. Se $\phi$ è non degenere, allora $F_{\phi}(U^\bot) = \Ann U$
	\end{itemize}

	\Altro{Aggiunto}
	\begin{itemize}
		\item $f^{*}$ è lineare
		\item $f^{**} = f$ (è un'involuzione)
		\item $\Ker f^{*} = (\Img f)^\bot$
		\item $\Img f^{*} = (\Ker f)^\bot$
		\item Se $\mathcal{B}$ è base di $V$, $A = \Mtr_{\mathcal{B}}(f), A^{*} = \Mtr_{\mathcal{B}}(f^{*}), M = \Mtr_{\mathcal{B}}(\phi)$, allora $A^{*} = M^{-1}{}^tAM$
	\end{itemize}

	\section*{Spazi Euclidei}
	Nel seguito avremo $(V, \varphi)$ spazio euclideo, ovvero con $\varphi$ definito positivo.
	\begin{itemize}
		\item In uno spazio euclideo non esistono vettori isotropi non nulli ($\varphi$ è definito positivo) ed esistono basi ortonormali
		\item ({\bf Ortonormalizzazione di Gram-Schmidt}) $\{v_1, \ldots, v_n\}$ base di $V$. Allora esiste una base ortonormale $\{w_1, \ldots, w_n\}$ di $V$ tale che $\Span(w_1, \ldots, w_j) = \Span(v_1, \ldots, v_j) \quad \forall j$
		\item ({\bf Ortogonabile triangolabilità}) $f \in \End(V)$ triangolabile, allora esiste $\mathcal{B}$ base di $V$ ortonormale ed a bandiera per $f$
		\item $f \in \End(V) \tc \varphi(f(x), f(y)) = \varphi(x, y) \quad \forall x, y \in V$, allora $f$ è un isomorfismo (ovvero è iniettivo e surgettivo)
		\item ({\bf Applicazioni Ortogonali}) $\cB$ base ortonormale di $(V, \varphi)$, $f \in End(V)$, $A = \mtrapp{f}{\cB}{\cB}$. Allora $f \in \Ort(V, \varphi) \sse {}^tA A = I$
		\item $(V, \varphi)$ euclideo, $\mathcal{B} = \{v_1, \ldots, v_n\}$ base ortonormale di $V$, $\mathcal{B'} = \{w_1, \ldots, w_n\}$ base di $V$, $M = \Mtr_{\mathcal{B'}, \mathcal{B}}(\Id)$. Allora $\mathcal{B'}$ è ortonormale $\sse M$ è ortogonale
		\item $A \in \mathcal{M}(n,\bbR)$ triangolabile. Allora $\exists M \in \Ort(n) \tc M^{-1}AM = T$ triangolare. Ovvero, se una matrice è triangolabile, allora si può triangolare anche con una matrice ortogonale.
		\item ({\bf Teorema Spettrale Reale}) $f$ è ortogonalmente diagonalizzabile $\sse f$ è autoaggiunto
		\item ({\bf Ortogonalizzazione Simultanea}) $V$ spazio vettoriale reale, $\varphi, \psi \in \PS(V)$, con $\varphi$ definito positivo. Allora $\exists \mathcal{B}$ base di $V$ ortonormale per $\varphi$ ed ortogonale per $\psi$
		\item ({\bf Simmetrica $\implies$ Ortogonalmente Diagonalizzabile}) $A \in \Symm(n, \bbR)$ matrice simmetrica {\it ad entrate reali}. Allora $\exists P \in \Ort(n) \tc P^{-1}AP = {}^tPAP = D$ diagonale.
		\item Una matrice simmetrica {\it ad entrate reali} è definita positiva $\sse$ ha tutti gli autovalori positivi
		\item ({\bf Ortogonalità degli Autospazi per applicazioni Autoaggiunte}) Se $f = f^{*}$, $\lambda \neq \mu$ autovalori per $f$, allora $V_\lambda \bot V_\mu$
		\item $A \in \mathcal{M}(n, \bbR)$. Allora $A$ è simmetrica $\sse A {}^tA = {}^tAA$ e $A$ è triangolabile
		\item ({\bf Radice Quadrata}) $A$ matrice simmetrica {\it ad entrate reali}. Allora $A$ è definita positiva $\sse \exists ! S$ simmetrica definita positiva $\tc A = S^2$
		\item ({\bf Decomposizione Polare}) $A \in \GL(n, \bbR)$. Allora $\exists ! S \in \Symm(n, \bbR)$ definita positiva e $P \in \Ort(n) \tc A = SP$ (decomposizione polare)
		\item ({\bf Simultanea Ortogonabile Diagonalizzabilità}) $f, g$ endomorfismi autoaggiunti $\tc f \circ g = g \circ f$. Allora esiste una base ortonormale di $V$ fatta da autovettori sia per $f$ che per $g$
	\end{itemize}

	\Altro{Isometrie di uno spazio Euclideo} \\
	In questa sezione $f: V \rightarrow V$ è una generica applicazione (NON per forza lineare) \\
	Sono fatti equivalenti:
	\begin{itemize}
		\item $f \in \Ort(V, \varphi)$
		\item $f \in \End(V)$ e $\norma{f(v)} = \norma{v} \forall v \in V$
		\item $f(0)=0$ e $d(f(x), f(y)) = d(x, y) \forall x, y \in V$
		\item $f \in \End(V)$ e $\forall \{v_1, \ldots, v_n\}$ base ortonormale di $V$, $\{f(v_1), \ldots, f(v_n)\}$ è base ortonormale di $V$
		\item $f \in \End(V)$ e $\exists \{v_1, \ldots, v_n\}$ base ortonormale di $V \tc \{f(v_1), \ldots, f(v_n)\}$ è base ortonormale di $V$
		\item $f \in \End(V)$ e $f^{*} \circ f = \Id$
	\end{itemize}

	\section*{Spazi Affini e Affinità}
	\begin{itemize}
		\item ({\bf Caratterizzazione delle applicazioni affini}) $f: A \rightarrow B$ è affine $\sse \exists \varphi: V \rightarrow W$ lineare tale che $\forall P_0 \in A$ valga $f(P) = f(P_0) + \varphi(\overrightarrow{P_0P})$
		\item ({\bf Grassmann Affine}) $\Dim(H+L) = \Dim(H) + \Dim(L) + 1 - \Dim(W_H \cap W_L)$
		\item Un'affinità in $\bbK^n$ si scrive come $f(X) = MX+N$, con $M \in \GL(n, \bbK)$ e $N \in \bbK^n$
		\item Il gruppo delle affinità in $\bbK^n$ si può vedere come sottogruppo di $\GL(n+1, \bbK)$ attraverso l'omomorfismo $(X \mapsto MX + N) \mapsto \left( \begin{array}{c|c} M & N \\ \hline 0 & 1 \end{array} \right)$
		\item ({\bf Esistenza ed Unicità dell'Affinità}) Due qualunque $(k+1)$-uple di punti di $\bbK^n$ affinemente indipendenti $F_1=\{P_0, \ldots, P_k\}$ e $F_2 = \{Q_0, \ldots, Q_k\}$ sono affinemente equivalenti, ovvero esiste (ed è unica) $g \in \Aff(\bbK^n) \tc g(P_i) = Q_i \quad \forall i$
	\end{itemize}

	\section*{Elementi di Geometria Affine Euclidea in $\bbR^n$, $\scal{\cdot}{\cdot}$}
	Nel seguito $S$ e $S'$ sono sottospazi affini di $\bbR^n$
	\Altro{Relazione di Ortogonalità}
	\begin{itemize}
		\item ({\bf Ortogonalità tra sottospazi affini}) $S$, $S'$ sottospazi affini di $\bbR^n$. Si dice che $S$ e $S'$ sono ortogonali ($S \bot S' \sse W_s \subseteq W_{S'}^\bot \sse W_{S'} \subseteq W_{S}^\bot$)
		\item Se $H$ è l'iperpiano di equazione $B\cdot X + d = 0$, allora $B$ è $\bot$ ad $H$		
		\item ({\bf Criterio di ortogonalità per Rette}) $r$, $r'$ rette di equazioni parametriche rispettivamente $X=At+C$, $X=A't+C'$ ($t \in \bbR$). Allora $r \bot r' \sse A\cdot A' = 0$
		\item ({\bf Criterio di ortogonalità Retta - Iperpiano}) $r$ retta di equazione $X=At+C$ ($t \in \bbR$), $H$ iperpiano di equazione $B \cdot X + d = 0$. Allora $r \bot H \sse A \parallel B$
		\item ({\bf Criterio di ortogonalità tra Iperpiani}) $H$, $H'$ iperpiani di equazioni rispettivamente $B \cdot X + d = 0$, $B' \cdot X + d' = 0$. Allora $H \bot H' \sse B \cdot B' = 0$
		\item Se $S' \bot S$ con $\Dim S = k$ allora $\Dim S' \le n-k$
		\item $\forall d \in \{0, \ldots, n-k\} \quad \exists S'$ sottospazio affine di $\bbR^n \tc S' \bot S$ e $\Dim S' = d$
		\item Tutti i sottospazi affini $S'$ di $\bbR^n \tc S' \bot S$ e $\Dim S' = n-k$ sono paralleli fra loro e ciascuno di essi interseca $S$ in uno ed un solo punto
		\item $\forall P \in \bbR^n \quad \exists ! S'$ sottospazio affine $\tc S' \bot S$ e $\Dim S' = n-k$ e $P \in S'$
		\item ({\bf Iperpiano ortogonale ad una retta e passante per un punto}) $r$ retta, $P \in \bbR^n$, Allora $\exists ! H$ iperpiano passante per $P$ ed ortogonale ad $r$. Tale piano interseca $r$ in uno ed un solo punto $P_0$. Se $r$ ha equazione parametrica $X = At+C$ allora $H$ ha equazione cartesiana $A\cdot X = A \cdot P$
	\end{itemize}

	\Altro{Distanza tra Sottospazi Affini}
	\begin{itemize}
		\item {\bf Definizione}: $P \in \bbR^n$. $d(P, S) = \inf \{d(P,X) \mid X \in S \}$
		\item ({\bf Distanza Punto - Iperpiano}) $H$ iperpiano di equazione $B \cdot X + d = 0$, $P \in \bbR^n$. Allora $d(P, H) = \frac{\mid B \cdot P + d \mid}{\norma{B}}$
		\item ({\bf Distanza tra due Rette}) \\ Se $r_1 \cap r_2 \neq \emptyset$ allora $d(r_1, r_2) = 0$ \\ Se $r_1 \parallel r_2$ allora $d(r_1, r_2) = d(P, r_2) \quad \forall P \in r_1$ \\ Se le due rette sono sghembe $r_1 = \{X \mid X = A_1t+C_1, t \in \bbR\}$, $r_2 = \{X \mid X = A_2t+C_2, t \in \bbR\}$. Voglio trovare una retta $l$ perpendicolare sia ad $r_1$ che ad $r_2$ e tale che le intersechi entrambe. Voglio quindi due punti $t_0$ e $\theta_0$ che risolvano $$\left( \begin{array}{cc} A_1\cdot A_1 & -A_2\cdot A_1 \\ A_1\cdot A_2 & -A_2\cdot A_2 \end{array} \right)\left(\begin{array}{c} t_0 \\ \theta_0 \end{array}\right) = \left( \begin{array}{c} C_2\cdot A_2 -C_1\cdot A_1 \\ C_2 \cdot A_2 - C_1 \cdot A_2 \end{array}\right)$$ Questo sistema ha sempre soluzione, poichè se le rette $r_1$ e $r_2$ sono sghembe la matrice è invertibile.
	\end{itemize}

	%% Aggiungere quante riflessioni esattamente servono, a seconda del luogo dei punti fissi
	\section*{Isometrie}
	Nel seguito supponiamo che $f$ sia un'isometria di $V$. Diamo le definizioni dei vari tipi di isometrie fondamentali
	\begin{itemize}
		\item Chiamiamo Isometrie le funzioni in $\Isom(V,d) = \{f: V \rightarrow V \mid d(P,Q) = d(f(P),f(Q)) \quad \forall P, Q \in V \}$
		\item $f$ è una {\bf Simmetria} se $f^2 = \Id$
		\item $f$ è una {\bf Riflessione} se $f^2 = \Id$ e $\Fix(f)$ è un iperpiano affine, ovvero $\Dim \Giac \Fix (f) = n-1$
		\item $f$ è una {\bf Rotazione} se $\Dim \Giac \Fix (f) = n-2$
		\item $f$ è una {\bf Glissoriflessione} (o {\bf Glide}) se è composizione di una riflessione $\rho$ e di una traslazione parallela a $\Fix(\rho)$
		\item $f$ è una {\bf Riflessione Rotatoria} se è composizione di una riflessione $\rho$ e di una rotazione attorno ad una retta ortogonale a $\Fix(\rho)$
		\item $f$ è un'{\bf Avvitamento} (o {\bf Twist}) se è composizione di una rotazione $R$ e di una traslazione (non banale) parallela a $\Fix(R)$
	\end{itemize}
	Ora qualche teorema sulle Isometrie:
	\begin{itemize}
		\item Ogni $f$ isometria si scrive $f(X) = AX + B$, con $A \in \Ort(n, \bbR)$, $B \in \bbR^n$
		\item ({\bf Decomposizione in Riflessioni}) Ogni $f \in \Isom(\bbR^n)$ è composizione di al più $n+1$ riflessioni
		\item $f(X) = AX+B$ è diretta se è composizione di un numero pari di riflessioni $\sse \Det(A) = 1$. Si dice inversa se è composizione di un numero dispari di riflessioni $\sse \Det(A) = -1$.
	\end{itemize}

	\section*{Classificazione delle Isometrie in $\bbR^2$ e $\bbR^3$}
	\Altro{Isometrie in $\bbR^2$} \\
	Ogni isometria di $\bbR^2$ è una Traslazione, una Rotazione, una Riflessione oppure una Glissoriflessione \vskip 1em

	\noindent\begin{tabular}{cccl}
	{\bf Nome} & {\bf Punti Fissi} & {\bf Tipo} & {\bf Composta da} \\
	Traslazione & $\emptyset$ & Diretta & Traslazione\\
	Rotazione & $\{ P \}$ & Diretta & 2 Riflessioni\\
	Riflessione & Retta & Inversa & Riflessione\\
	Glissoriflessione & $\emptyset$ & Inversa & Riflessione $\rho$ +\\ 
	(o Glide)	&		&	&traslazione $\parallel$ a Fix($\rho$)\\
	\end{tabular} \vskip 1em


	\Altro{Isometrie in $\bbR^3$} \\
	Ogni isometria di $\bbR^3$ è una Traslazione, una Rotazione, una Riflessione, una Glissoriflessione, un Avvitamento o una Riflessione Rotatoria\vskip 1em

	\noindent\begin{tabular}{cccl}
	{\bf Nome} & {\bf Punti Fissi} & {\bf Tipo} & {\bf Composta da} \\
	Traslazione & $\emptyset$ & Diretta & Traslazione\\
	Rotazione & Retta & Diretta & 2 Riflessioni\\
	Riflessione & Piano & Inversa & Riflessione\\
	Glissoriflessione & $\emptyset$ & Inversa & Riflessione $\rho$ +\\
	(o Glide)	&		&	&traslazione $\parallel$ a Fix($\rho$)\\
	Avvitamento & $\emptyset$ & Diretta & Rotazione $R$\\
	(o Twist)	&		&	&traslazione $\parallel$ a Fix($R$)\\
	Riflessione & $\{ P\}=\Fix(R)\cap\Fix(\rho)$ & Inversa & Riflessione $\rho$ + Rotazione $R$ \\ 
	Rotatoria & 					& 	&  $\tc \Fix(\rho)\bot\Fix(R)$\\
	\end{tabular} \vskip 1em

	\section*{Coniche e Quadriche}
	\begin{itemize}
		\item Una Quadrica $\mathcal{Q}$ di equazione ${}^tXAX +2({}^tBX) +c = 0$ in $\bbK^n$ si può immergere in $\bbK^{n+1}$ attraverso la matrice $\widetilde{Q} = \left( \begin{array}{c|c} A & B \\ \hline {}^tB & c \end{array} \right) $
		\item $\mathcal{Q} = [f]$, $\widetilde{X} = \left(\begin{array}{c} X\\ \hline 1 \end{array} \right)$. Allora $f(X)= 0 \ \sse \ {}^t\widetilde{X}\widetilde{Q}\widetilde{X}=0$
		\item Sia $g \in \Aff(\bbK^n)$, $g(X) = MX + N$ può essere vista come applicazione lineare in $\bbK^{n+1}$ la cui matrice associata è $\widetilde{M}_N = \left(\begin	{array}	{c|c} M & N\\ \hline 0 & 1\end{array} \right)$.
		Inoltre, detta $\widetilde{H}$ la matrice di immersione in $\bbK^{n+1}$ di $\mathcal{H} = [f \circ g]$, si ha che $\widetilde{H} = {}^t\widetilde{M}_N\widetilde{Q}\widetilde{M}_N$
		\item $\widetilde{H} = {}^t\widetilde{M}_N\widetilde{Q}\widetilde{M}_N = \left( \begin{array}{c|c} {}^tMAM & {}^tM(AN + B)\\ \hline {}^t({}^tM(AN + B)) & {}^tNAN + 2({}^tNB) + c \end{array} \right)$
		\item ({\bf Esistenza di un Centro}) Una quadrica $\mathcal{C}$ è a centro $\sse$ il sistema $AT+B=0$ è risolubile (ovvero $\exists T \in \bbK^n$ che lo risolve)
	\end{itemize}

	\section*{Classificazione delle Coniche in $\bbR^2$ e $\bbC^2$}
	\Altro{Coniche in $\bbR^2$} \vskip .5em
	\noindent \begin{tabular}{cllc}
	{\bf Conica} & {\bf Equazione} & {\bf Nome} & {\bf $(\Rk A, \Rk Q, \omega(A), \omega(Q))$} \\
	1 & $x^2-y=0$ & Parabola & $(1, 3, 1, 1)$ \\
	2 & $x^2+y^2+1=0$ & Ellisse Immaginaria & $(2, 3, 0, 0)$ \\
	3 & $x^2+y^2-1=0$ & Ellisse Reale & $(2, 3, 0, 1)$ \\
	4 & $x^2-y^2+1=0$ & Iperbole & $(2, 3, 1, 1)$ \\
	5 & $x^2+y^2=0$ & Rette Complesse Incidenti & $(2, 2, 0, 1)$ \\
	6 & $x^2-y^2=0$ & Rette Incidenti & $(2, 2, 1, 2)$ \\
	7 & $x^2+1=0$ & Rette Complesse Parallele & $(1, 2, 1, 1)$ \\
	8 & $x^2-1=0$ & Rette Parallele & $(1, 2, 1, 2)$ \\
	9 & $x^2=0$ & Retta Doppia & $(1, 1, 1, 2)$ \\
	\end{tabular} \vskip 1em

	\newpage
	\Altro{Coniche in $\bbC^2$} \vskip .5em
	\noindent \begin{tabular}{cllc}
	{\bf Conica} & {\bf Equazione} & {\bf Nome} & {\bf $(\Rk A, \Rk Q)$} \\
	1 & $x^2-y=0$ & Parabola & $(1, 3)$ \\
	2 & $x^2+y^2+1=0$ & Ellisse & $(2, 3)$ \\
	3 & $x^2+y^2=0$ & Rette Incidenti & $(2, 2)$ \\
	4 & $x^2+1=0$ & Rette Parallele & $(1, 2)$ \\
	5 & $x^2=0$ & Retta Doppia & $(1, 1)$ \\
	\end{tabular} \vskip 1em

	\section*{Classificazione delle Quadriche in $\bbR^3$}
	\Altro{Quadriche in $\bbR^3$} \vskip .5em
	\noindent \begin{tabular}{cllc}
	{\bf Quadrica} & {\bf Equazione} & {\bf Nome} & {\bf $(\Rk A, \Rk Q, \omega(A), \omega(Q))$} \\
	1 & $x^2+y^2+z^2+1=0$ & Ellissoide Immaginario & $(3, 4, 0, 0)$ \\
	2 & $x^2+y^2+z^2-1=0$ & Ellissoide & $(3, 4, 0, 1)$ \\
	3 & $x^2+y^2-z^2-1=0$ & Iperboloide a una falda & $(3, 4, 1, 2)$ \\
	4 & $x^2+y^2-z^2+1=0$ & Iperboloide a due falde & $(3, 4, 1, 1)$ \\
	5 & $x^2+y^2+z^2=0$ & Cono Immaginario & $(3, 3, 0, 1)$ \\
	6 & $x^2+y^2-z^2=0$ & Cono Reale & $(3, 3, 1, 2)$ \\
	7 & $x^2+y^2=0$ & Piani Complessi Incidenti & $(2, 2, 1, 2)$ \\
	8 & $x^2-y^2=0$ & Piani Incidenti & $(2, 2, 2, 3)$ \\
	9 & $x^2=0$ & Piano Doppio & $(1, 1, 2, 3)$ \\
	10 & $x^2+y^2+1=0$ & Cilindro Immaginario & $(2, 3, 1, 1)$ \\
	11 & $x^2-y^2+1=0$ & Cilindro Iperbolico & $(2, 3, 2, 2)$ \\
	12 & $x^2+y^2-1=0$ & Cilindro Ellittico & $(2, 3, 1, 2)$ \\
	13 & $x^2-1=0$ & Piani Paralleli & $(1, 2, 2, 3)$ \\
	14 & $x^2+1=0$ & Piani Complessi Paralleli & $(1, 2, 2, 2)$ \\
	15 & $x^2+y^2-z=0$ & Paraboloide Ellittico & $(2, 4, 1, 1)$ \\
	16 & $x^2-y^2-z=0$ & Paraboloide Iperbolico (Sella) & $(2, 4, 2, 2)$ \\
	17 & $x^2-z=0$ & Cilindro Parabolico & $(1, 3, 2, 2)$ \\
	\end{tabular} \vskip 1em

	\section*{Classificazione delle Quadriche in $\bbR^n$ e $\bbC^n$}
	\Altro{Quadriche in $\bbR^n$} \\
	$p = i_{+}(A)$, $r = \Rk A$ \vskip .5em
	\noindent \begin{tabular}{lll}
	{\bf Equazione} & {\bf Nome} & {\bf Note} \\
	$x_1^2+\ldots+x_p^2-x_{p+1}^2-\ldots-x_r^2+d=0$, con $d=0,1$ & A centro & $\Rk Q = d + \Rk A$ \\
	$x_1^2+\ldots+x_p^2-x_{p+1}^2-\ldots-x_r^2-x_n=0$ & Paraboloide & $\Rk Q = 2 + \Rk A$ \\
	\end{tabular} \vskip 1em

	\Altro{Quadriche in $\bbC^n$} \vskip .5em
	\noindent \begin{tabular}{lll}
	{\bf Equazione} & {\bf Nome} & {\bf Note} \\
	$x_1^2+\ldots+x_r^2+d=0$, con $d=0,1$ & A centro & $r = \Rk A$, $\Rk Q = d + \Rk A$ \\
	$x_1^2+\ldots+x_r^2-x_n=0$ & Paraboloidi & $r = \Rk A$, $\Rk Q = 2 + \Rk A$ \\
	\end{tabular} \vskip 1em

	\section*{Forme Canoniche di Matrici Speciali}
	\Altro{Forma canonica delle Matrici Ortogonali in $(\bbR^n, \langle \cdot \mid \cdot \rangle)$} \\
	Ogni matrice ortogonale $R \in O(V, \langle \cdot \mid \cdot \rangle)$ è tale che ${}^tRR = I$ e per ogni $\lambda$ autovettore si ha $\norma{\lambda} = 1$. Inoltre, una matrice $M$ è ortogonale $\sse$ le righe (e le colonne) di $M$ formano una base ortonormale di $\bbR^n$. $\Det M = \pm 1$ \\
	La forma canonica è $R = \left( \begin{array}{ccccccccc} 1 & & & & & & & & \\ & \ddots & & & & & & & \\ & & 1 & & & & & & \\ & & & -1 & & & & & \\ & & & & \ddots & & & & \\ & & & & & -1 & & & \\ & & & & & & R_{\theta_1} & & \\ & & & & & & & \ddots & \\ & & & & & & & & R_{\theta_k} \\ \end{array} \right)$, dove gli $R_\theta$ sono matrici $2 \times 2$ che si scrivono come $R_{\theta_i} = \left( \begin{array}{cc} \cos \theta_i & \sin \theta_i \\ - \sin \theta_i & \cos \theta_i \\ \end{array} \right)$

	\section*{Come trasformano le Cose?}
	\Altro{Matrici che rappresentano applicazioni lineari} \\
	Sia $f: V \rightarrow W$ un'applicazione lineare, $F_{\mathcal{B}} = \mtrapp{f}{\cB}{\cB}$. Se cambio base alla matrice dell'applicazione da $\mathcal{B}$ a $\mathcal{S}$, ovvero se voglio scrivere $F_{\mathcal{S}} = \mtrapp{f}{\cS}{\cS}$ rispetto a $F_{\mathcal{B}}$, devo costruire una matrice $M$ che mangia coordinate pensate in base $\mathcal{S}$ e le sputa pensate in base $\mathcal{B}$. \\
	Voglio cioè $M = \left( \begin{array}{c|c|c}  & & \\ {[s_1]}_{\mathcal{B}} & \cdots & {[s_n]}_{\mathcal{B}} \\  &  &  \\ \end{array} \right) = \revmtrapp{\Id}{\cS}{\cB}$ \\
	Allora si ha $F_{\mathcal{S}} = M^{-1}F_{\mathcal{B}}M$

	\Altro{Matrici che rappresentano prodotti scalari} \\
	Sia $\varphi: V \times V \rightarrow \bbK$ un prodotto scalare, $\Phi_{\mathcal{B}} = \Mtr_{\mathcal{B}}(\varphi)$. Se cambio base alla matrice del prodotto scalare da $\mathcal{B}$ a $\mathcal{S}$, ovvero se voglio scrivere $\Phi_{\mathcal{S}} = \Mtr_{\mathcal{S}}(\varphi)$ rispetto a $\Phi_{\mathcal{B}}$, devo costruire una matrice $M$ che mangia coordinate pensate in base $\mathcal{S}$ e le sputa pensate in base $\mathcal{B}$. \\
	Voglio cioè $M = \left( \begin{array}{c|c|c} & & \\ {[s_1]}_{\mathcal{B}} & \cdots & {[s_n]}_{\mathcal{B}} \\ & & \\ \end{array} \right) = \revmtrapp{\Id}{\cS}{\cB}$ \\
	Allora si ha $\Phi_{\mathcal{S}} = {}^tM\Phi_{\mathcal{B}}M$
	
	\Altro{Coniche attraverso Affinità} \\
	Sia $\Psi(X) = MX+N$ un'affinità. Se scrivo $\Psi(\mathcal{Q}) = \mathcal{C}$ sto intendendo che "il supporto di $\mathcal{Q}$" va a finire in $\mathcal{C}$ se gli applico l'affinità, mentre le equazioni trasformano con l'inversa dell'affinità. Quindi se $\mathcal{Q} = \{(x,y) \mid g(x,y) = 0\}$ allora $\mathcal{C} = \{(x,y) \mid g(\Psi^{-1}(x,y)) = 0\}$

	\section*{Identità e Fatti Utili}
	\begin{itemize}
		\item $E_{ij}^{\alpha\beta} := \delta_{\alpha i} \delta_{\beta j}$ e sono una base dello spazio vettoriale $\mathcal{M}(n,\bbK)$
		\item $E^{\alpha\beta} + E^{\beta\alpha}$ sono una base delle matrici simmetriche
		\item $E^{\alpha\beta} - E^{\beta\alpha}$ sono una base delle matrici antisimmetriche
		\item Le $(I + E^{\alpha\beta})$ con $\alpha \neq \beta$ sono matrici invertibili molto semplici (le matrici invertibili non sono un sottospazio, quindi non ha senso parlare di base). $(I - E^{\alpha\beta})$ è l'inversa
		\item $E^{\alpha\beta}E^{\rho\sigma} = \delta_{\beta\rho} E^{\alpha\sigma}$
		\item $[E^{\alpha\beta}A]_{ij} = \delta_{i\alpha}A_{\beta j}$ (porta la $\beta$-esima riga di $A$ nella $\alpha$-esima riga della matrice prodotto)
		\item $[AE^{\alpha\beta}]_{ij} = \delta_{j\beta}A_{i\alpha}$ (porta la $\alpha$-esima colonna di $A$ nella $\beta$-esima colonna della matrice prodotto)
		\item $[E^{\alpha\beta}AE^{\rho\sigma}]_{ij} = \delta_{i\alpha}\delta_{\sigma j}A_{\beta\rho} = cE^{\alpha\sigma}$ dove $c = [A]_{\beta\rho}$ (quindi esce una matrice che ha l'elemento $A_{\beta\rho}$ come unico elemento non nullo nel posto $\alpha\sigma$)
		\item $A = \left( \begin{array}{cc} a & b \\ c & d \end{array} \right)$ (matrice $2\times 2$) allora si ha $A^{-1} = \frac{1}{bc-ad} \left( \begin{array}{cc} -d & b \\ c & -a \end{array} \right)$
		
		\item $A \in \mathcal{M}(n, \bbK) \tc AX = XA \quad \forall X \in \mathcal{M}(n, \bbK)$ sono del tipo $A = \lambda I$ (si dimostra mettendo al posto di $X$ tutte le $E^{\alpha\beta}$)
		\item $A \in \mathcal{M}(n, \bbK) \tc AD = DA$, {\bf con $\bm D$ matrice diagonale fissata} soddisfano $A_{ij}(\lambda_i - \lambda_j) = 0 \quad \forall i,j$ dove $D_{ij} = \delta_{ij}\lambda_i$. Cioè se chiamiamo $\mu_1, \ldots, \mu_k$ gli autovalori di $D$ (senza molteplicità), e riordiniamo la base di D in modo che siano in ordine crescente, abbiamo $$D = \left(\begin{array}{c|c|c} M_{\mu_1} & 0 & 0 \\ \hline 0 & \ddots & 0 \\ \hline 0 & 0 & M_{\mu_k} \end{array}\right) \ \ \ \ \ \ A = \left(\begin{array}{c|c|c} \star & 0 & 0 \\ \hline 0 & \ddots & 0 \\ \hline 0 & 0 & \star \end{array}\right) $$ con $M_{\mu_i} = \left(\begin{array}{ccc} \mu_i & 0 & 0 \\ 0 & \ddots & 0 \\ 0 & 0 & \mu_i \end{array}\right)$
		\item Le matrici di un certo rango fissato $r > 0$ (qualunque) generano come spazio vettoriale tutte le matrici $n\times n$ (Le $E^{\alpha\beta}$ vengono generate tutte piuttosto in fretta)
		\item $\Dim \mbox{ Simmetriche } = \frac{n(n+1)}{2}$, $\Dim \mbox{ Antisimmetriche } = \frac{n(n-1)}{2}$
	\end{itemize}
	
\end{document}
